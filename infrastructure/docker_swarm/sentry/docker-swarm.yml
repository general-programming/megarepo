version: "3.6"

x-restart-policy: &restart_policy
  restart: always
  deploy:
    restart_policy:
      condition: any

x-sentry-defaults: &sentry_defaults
  << : *restart_policy
  build:
    context: .
    args:
      - SENTRY_IMAGE
  image: registry.generalprogramming.org/sentry
  depends_on:
    - redis
    - memcached
    - snuba-api
    - snuba-consumer
    - snuba-outcomes-consumer
    - snuba-replacer
    - symbolicator
  environment:
    SNUBA: 'http://snuba-api:1218'
  volumes:
    - '/srv/var/sentry/data:/data'
  networks:
    - sentry
  env_file: .env

x-snuba-defaults: &snuba_defaults
  << : *restart_policy
  depends_on:
    - redis
    - clickhouse
    - kafka
  image: 'getsentry/snuba:latest'
  environment:
    SNUBA_SETTINGS: docker
    CLICKHOUSE_HOST: clickhouse
    DEFAULT_BROKERS: 'kafka:9092'
    REDIS_HOST: redis
    UWSGI_MAX_REQUESTS: '10000'
    UWSGI_DISABLE_LOGGING: 'true'
  networks:
    - sentry

services:
  memcached:
    << : *restart_policy
    image: 'memcached:1.5-alpine'
    networks:
      - sentry

  redis:
    << : *restart_policy
    image: redis:alpine
    command: 'redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru --save ""'
    networks:
      - sentry

  minio:
    << : *restart_policy
    image: minio/minio
    command: server /data
    volumes:
      - /srv/var/sentry/minio:/data
    environment:
      MINIO_ACCESS_KEY: SENTRY_MINIO_INTERNAL
      MINIO_SECRET_KEY: MINIO_INSECURE_SECRET
    networks:
      - sentry

  zookeeper:
    << : *restart_policy
    image: 'confluentinc/cp-zookeeper:5.1.2'
    environment:
      ZOOKEEPER_CLIENT_PORT: '2181'
      CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: 'WARN'
      ZOOKEEPER_TOOLS_LOG4J_LOGLEVEL: 'WARN'
    volumes:
      - '/srv/var/sentry/zookeeper:/var/lib/zookeeper/data'
      - '/srv/var/sentry/zookeeper-log:/var/lib/zookeeper/log'
      - '/srv/var/sentry/secrets:/etc/zookeeper/secrets'
    networks:
      - sentry

  kafka:
    << : *restart_policy
    depends_on:
      - zookeeper
    image: 'confluentinc/cp-kafka:5.1.2'
    environment:
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1'
      CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
      KAFKA_LOG4J_LOGGERS: 'kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,kafka.zookeeper=WARN,state.change.logger=WARN'
      KAFKA_LOG4J_ROOT_LOGLEVEL: 'WARN'
      KAFKA_TOOLS_LOG4J_LOGLEVEL: 'WARN'
    volumes:
      - '/srv/var/sentry/kafka:/var/lib/kafka/data'
      - '/srv/var/sentry/kafka-log:/var/lib/kafka/log'
      - '/srv/var/sentry/secrets:/etc/kafka/secrets'
    networks:
      - sentry

  clickhouse:
    << : *restart_policy
    image: 'yandex/clickhouse-server:19.11'
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - '/srv/var/sentry/clickhouse:/var/lib/clickhouse'
    networks:
      - sentry

  snuba-api:
    << : *snuba_defaults

  snuba-consumer:
    << : *snuba_defaults
    command: consumer --dataset events --auto-offset-reset=latest --max-batch-time-ms 750

  # Kafka consumer responsible for feeding outcomes into Clickhouse
  # Use --auto-offset-reset=earliest to recover up to 7 days of TSDB data
  # since we did not do a proper migration
  snuba-outcomes-consumer:
    << : *snuba_defaults
    command: consumer --dataset outcomes --auto-offset-reset=earliest --max-batch-time-ms 750

  snuba-replacer:
    << : *snuba_defaults
    command: replacer --auto-offset-reset=latest --max-batch-size 3

  snuba-cleanup:
    << : *snuba_defaults
    image: registry.generalprogramming.org/snuba-cleanup-onpremise-local
    build:
      context: ./cron
      args:
        BASE_IMAGE: 'getsentry/snuba:latest'
    command: '"*/5 * * * * gosu snuba snuba cleanup --dry-run False"'

  symbolicator:
    << : *restart_policy
    image: 'getsentry/symbolicator:latest'
    volumes:
      - '/srv/var/sentry/symbolicator:/data'
    command: run

  symbolicator-cleanup:
    << : *restart_policy
    image: registry.generalprogramming.org/symbolicator-cleanup-onpremise-local
    build:
      context: ./cron
      args:
        BASE_IMAGE: 'getsentry/symbolicator:latest'
    command: '"55 23 * * * gosu symbolicator symbolicator cleanup"'
    volumes:
      - '/srv/var/sentry/symbolicator:/data'

  web:
    << : *sentry_defaults
    networks:
      - sentry
      - publicweb
    deploy:
      labels:
        - traefik.enable=true
        - traefik.port=9000
        - traefik.docker.network=publicweb
        # Routes
        - traefik.frontend.rule=Host:sentry.generalprogramming.org

  cron:
    << : *sentry_defaults
    command: run cron

  worker:
    << : *sentry_defaults
    command: run worker

  post-process-forwarder:
    << : *sentry_defaults
    # Increase `--commit-batch-size 1` below to deal with high-load environments.
    command: run post-process-forwarder --commit-batch-size 1

  sentry-cleanup:
    << : *sentry_defaults
    image: registry.generalprogramming.org/sentry-cleanup-onpremise-local
    build:
      context: ./cron
      args:
        BASE_IMAGE: 'registry.generalprogramming.org/sentry'
    command: '"0 0 * * * gosu sentry sentry cleanup --days $SENTRY_EVENT_RETENTION_DAYS"'

networks:
  sentry:
    driver: overlay
  publicweb:
    driver: overlay
    external: true
